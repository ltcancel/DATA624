---
title: "Data624 - Project 2"
author: "Esteban Aramayo, Coffy Andrews-Guo, LeTicia Cancel, Joseph Connolly, Ian Costello"
date: '7/16/2022'
output: 
  word_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE,
                      message=FALSE,
                      collapse = FALSE,
                      comment = "#>" )
```

# Project Summary

For this project, we are tasked to predict the "potential of hydrogen" more commonly known as pH in a range of beverages. Two data sets are provided one for modeling and the other to use for predictions. The modeling data set contains 33 different features, including the target variable (`PH`). The predictor variables are all numeric except for `Brand Code`, which has a categorical. 

## Definition of pH

The pH of a solution is meant to describe its acidity or alkalinity (basic). The pH scale ranges from zero to fourteen, where values closer to zero are extremely acidic and usually quite dangerous if handled improperly. Values closer to fourteen are more basic, that is alkaline, and are **also** quite dangerous if handled improperly. Values in the middle of the scale usually defined as "around seven" are so-called neutral. Pure water, milk, sea water, and human saliva are often examples of neutral substances though can tend slightly either alkaline or acidic.

## Libraries Used

* Standard libraries applied (e.g., `tidyverse`, `psych`, `ggplot2`)
* Portions of the data exploration where assisted by the `DataExplorer` package
* Of particular use throughout the project is the `caret` package that provides for flexible pre-processing, modeling options, and tuning controls. 

### CODE Reproducibility / Repeatability

The reproducibility and repeatability of the analysis is important on building upon findings or providing comparison results. The datasets are structured and the machine learning models (classification and regression), will require a broad approach on code organizing, code reporting, and many other features.  Learning best practices for writing reproducible code is an iterative learning process on repeating steps to troubleshoot, modify, and gain insights for accurate results. The `set.seed` function is set at `1234` to provide exact conditions to reproduce these processes.   

```{r warning=FALSE, message=FALSE}
library(readxl)
library(tidyverse)
library(ggplot2)
library(patchwork)
library(caret)
library(RANN)
library(VIM)
library(ggpubr)
library(gridExtra)
library(psych)
library(tidyverse)
library(ggplot2)
library(GGally)
library(caret)
library(broom)
library(kableExtra)
library(DataExplorer)
library(psych)
```

```{r}
set.seed(1234)
```

```{r}
raw_df <- read_csv("StudentData - TO MODEL.csv") %>%
  select(-"Brand Code")

#remove whitespace between strings in headers - column names
colnames(raw_df) <- gsub(" ","",colnames(raw_df))
```

## Data Exploration

The variable `Brand Code` is excluded as it is the only categorical variable which would require different operations (such as creating dummy variables) than the numeric columns. It also would seem that `Brand Code` would have little impact on pH of specific products. 

Using `describe` on the remaining features provides the first glimpse of the shape of these variables. Of note are the skew and kurtosis values, which would at first blush generally indicate (or not) a normal distribution. 

```{r}
describe(raw_df)
```

## Data Visualizations

### Feature Histograms

Variable histograms below better illustrate the shape of the feature distributions. The `PH` distribution appears to be normal, this is a requirement for linear regressions. For the other variables, the distributions are a mix. Some appear quite normal, such as `Carb Pressure`, `Carb Temp`, `Fill Ounces`. Others are bimodel, that is having two "peaks" in the distributions such as `Balling Lvl`, `Density`, `Carb Rel`. The third category of variable are those that may or may not have rather normal distributions but also have significant outliers such as `Hyd Pressure`, `Filler Speed`. 

```{r, warning = FALSE, message = FALSE, echo=FALSE}
DataExplorer::plot_histogram(raw_df)
```

### Box Plots

Using box plots will provide more insight into the outliers of these features.

```{r, fig.height = 12, fig.width = 10, echo=FALSE}
# Prepare data for ggplot
df_boxplots <- raw_df %>% 
  dplyr::select(-c(PH)) %>%
  tidyr::drop_na() %>%
  gather(key = 'variable', value = 'value')

# Boxplots for each variable
df_boxplots %>% ggplot() + 
  geom_boxplot(aes(x=variable, y=value), fill = "lightblue", 
               color = "darkblue", outlier.color = "red", outlier.alpha = 0.2) + 
  facet_wrap(. ~variable, scales='free', ncol=4)+
  labs(title = "Boxplot of all feature variables") + 
  theme(plot.title = element_text(hjust = 0.5, size = 12, face = "bold"))

```

### Correlation

```{r,echo=FALSE, fig.height=8, fig.width=10 }
suppressPackageStartupMessages(library(corrplot))
suppressPackageStartupMessages(library(RColorBrewer))
correlation = cor(raw_df, use = 'pairwise.complete.obs')
corrplot(correlation, 'ellipse', type = 'lower', order = 'hclust',
         col=brewer.pal(n=8, name="Dark2"))
```

### Relationship to Target

```{r}

par(mfrow = c(4,2))

#include target in the df for numeric data
histData <- raw_df 

#How do I color by Targetflag
featurePlot(x= histData[1:8], y = histData[['PH']])

featurePlot(x= histData[9:16], y = histData[['PH']])

featurePlot(x= histData[17:24], y = histData[['PH']])

featurePlot(x= histData[25:32], y = histData[['PH']])

#HOME KIDS and AGE NEED BAR CHARTS

```

## Data Preparation

```{r}
names(raw_df)[nearZeroVar(raw_df)]
```

### Missing Data and Imputation

```{r, warning = FALSE, message = FALSE, echo=FALSE}
#plot missing values using VIM package
aggr(raw_df , col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(raw_df), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
```

```{r}
preProc <- preProcess(as.data.frame(raw_df), method = c("BoxCox","center","scale","knnImpute"))
df <- predict(preProc, as.data.frame(raw_df))
```

## Feature Q-Q Plots

```{r, warning = FALSE, message = FALSE, echo=FALSE}
qq_data <- df

DataExplorer::plot_qq(qq_data, nrow = 4L, ncol = 3L)
```

```{r}
xTrain <- df %>%
  select(-PH)
yTrain <- df %>%
  select(PH)
```

```{r}
head(xTrain)
```


```{r, warning = FALSE, message = FALSE, echo=FALSE}
ggplot(stack(xTrain), aes(x = ind, y = values)) +
  geom_boxplot(color = "darkblue",
               fill = "lightblue",
               alpha = 0.2,
               outlier.color = "red",
               outlier.fill = "red",
               outlier.alpha = 0.2,
               notch = TRUE) + 
  labs(title = "Boxplot of all feature variables") + 
  scale_y_log10()
```

```{r}

tooHigh <- findCorrelation(cor(xTrain), cutoff = .75)

tooHigh
```

```{r}
xTrain <- xTrain[, -tooHigh]
trainXnnet <- xTrain[, -tooHigh]
```

# Model Building

## Repeated Cross Validation 

```{r}
ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 5)
```

## Models Tested

```{r}
lm_model <- train(xTrain, yTrain$PH, method="lm", trControl=ctrl)
lm_model
```

```{r}
knn_model <- train(xTrain, yTrain$PH, method="knn", trControl=ctrl)
knn_model
```

```{r nnet}

# Resource: Chapter 7.5 of textbook "Applied Predictive Modeling" by KJ

## Create a specific candidate set of models to evaluate:
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
                        .size = c(1:10),
                        .bag = FALSE)


nnetTune <- train(xTrain, yTrain$PH,
                  method = "avNNet",
                  tuneGrid = nnetGrid,
                  trControl = ctrl,
                  linout = TRUE,
                  trace = FALSE,
                  MaxNWts = 1 * (ncol(trainXnnet) + 10) + 10 + 1,
                  maxit = 50)

nnetTune
```

```{r mars-chem}

# Resource: Chapter 7.5 of textbook "Applied Predictive Modeling" by KJ
# Resource: http://uc-r.github.io/mars

library(earth)
library(dplyr)

## Create a specific candidate set of models to evaluate:
marsGrid <- expand.grid(degree = 1:3,
                        nprune = seq(2, 100, length.out = 10) %>% floor()
  )

# cross validated model
tuned_MARS <- train(
  x = xTrain,
  y = yTrain$PH,
  method = "earth",
  metric = "RMSE",
  trControl = ctrl,
  tuneGrid = marsGrid
)

tuned_MARS

```

```{r svm}

# Resource: Chapter 7.5 of textbook "Applied Predictive Modeling" by KJ

library(kernlab)

svmTuned <- train(xTrain, yTrain$PH,
                   method = "svmRadial",
                   preProc = c("center", "scale"),
                   tuneLength = 14,
                   trControl = ctrl)

svmTuned

```

```{r}
plot(svmTuned)
```

```{r}
rpart <- train(xTrain, yTrain$PH,
               method = "rpart2", 
               tuneLength = 10, 
               trControl = ctrl)
rpart
```

```{r}
gbmG <- expand.grid(.interaction.depth = seq(1, 7, by = 2),
                        .n.trees = seq(100, 1000, by = 100),
                        .shrinkage = c(0.01, 0.1),
                        .n.minobsinnode = 8)

gbmModel <- train(xTrain, yTrain$PH,
                  method = "gbm",
                  tuneGrid =gbmG,
                  trControl = ctrl,
                  verbose=F
                 )
gbmModel
```

```{r}
cubistTuned <- train(xTrain, yTrain$PH, 
                     method = "cubist", 
                     trControl = ctrl)

cubistTuned
```

## Model Results

# Model Selection










